<!DOCTYPE HTML>
<!--
    "Thanks to HTML5 UP for this template".
    Massively by HTML5 UP
    html5up.net | @ajlkn
    Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
    <head>
        <!-- Code Render-->
        <link rel="stylesheet" href="../data/syntaxhighlighter/shCore.css">
        <link rel="stylesheet" href="../data/syntaxhighlighter/shThemeDefault.css">
        <script type="text/javascript" src="../data/syntaxhighlighter/shCore.js"></script>
        <script type="text/javascript" src="../data/syntaxhighlighter/shBrushPython.js"></script>

        <!--MathJax-->
            <script type="text/x-mathjax-config">
                    MathJax.Hub.Config({
                                            config: ["MMLorHTML.js"],
                                            extensions: ["mml2jax.js"],
                                            jax: ["input/MathML"]
                                    });
            </script>

            <script type="text/javascript" async
            src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
            </script>
            
            <style>
                .math-container {
                overflow-x: auto;
                display: block;
            }
            </style>        

		<title>Singular Value Decomposition</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
        <link rel="stylesheet" href="../data/assets/css/main.css" />
        <noscript><link rel="stylesheet" href="../data/assets/css/noscript.css" /></noscript>
        <link rel="icon" href="../data/icon.png" type="image/png" sizes="16x16">
    </head>    
    
    <body class="is-preload">

        <!-- Wrapper -->
            <div id="wrapper">

                <!-- Header -->
                    <header id="header">
                        <a href="index.html" class="logo">Linear Algebra</a>
                    </header>

                <!-- Nav -->
                    <nav id="nav">
                        <ul class="links">
                            <li class="active"><a href="svd.html">Singular Value Decomposition</a></li>
                        </ul>
                        <ul class="icons">
                            <li><a href="https://quantml.org/" style="border-bottom-color: transparent;"><img style="margin-right: 5px; margin-left: 5px;" class="logo-nav" src="../data/icon.png" alt="logo" width="20" height="20"></a></li>
							<li><a href="https://join.slack.com/t/quantml-org/shared_invite/zt-hcmbg7fr-jPbVAUT_tjGPaKWU50qMYQ" class="fab fa-slack slack-nav" style="color: blue;"><span class="label"></span></a></li>
                            <li><a href="https://www.linkedin.com/in/yuvraj97/" class="icon brands fa-linkedin"><span class="label">Linkedin</span></a></li>
                            <li><a href="https://github.com/yuvraj97/" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
                        </ul>					</nav>
                <!-- Main -->
                    <div id="main">
                        <section class="post">
<div class='buttons'>    
    <button onclick="window.location.href = 'similar-matrices.html';">&#x25C0;&nbsp;&nbsp;Similar Matrices</button>
    <button style="float: right;" onclick="location.href = 'https://quantml.org/';"><img style="margin-top: 7px; " src="../data/icon.png" alt="logo" width="30" height="30"></button>
</div><br>

<p>
<h2>Singular Value Decomposition</h2>
<blockquote style="background-color:#f6f8fa; border-left-color:#fff; border-radius: 35px;">
<a style="color: royalblue;" href="diagonalization.html">Previously</a> we have seen that for <b>\(n\times n\)</b> 
matrix (say\(A\)) with \(n\) independent eigenvectors factorization is,
\[\bbox[5px,border:2px solid blue]{ \color{blue}{
    \begin{matrix}
        \\    %blank line
    \quad 
    
    % equation
    A=S\Lambda S^{-1}
    %equation
    
    \quad\\
        \\    %blank line
    \end{matrix}
    }}
\]
<a style="color: royalblue;" href="orthonormal-vectors.html">Previously</a> we have seen that if eigenvectors of
matrix \(A\) are <b>orthogonal</b> then \(Q^{-1}=Q^T\), then the factorization is,<br>
(We discussed it in detail <a style="color: royalblue;" href="symmetric-matrices.html">HERE</a>)

\[\bbox[5px,border:2px solid blue]{ \color{blue}{
    \begin{matrix}
        \\    %blank line
    \quad 
    
    % equation
    A=Q\Lambda Q^{T}
    %equation
    
    \quad\\
        \\    %blank line
    \end{matrix}
    }}
\]

But generally eigenvectors are not orthogonal.
</blockquote><BR>

<b>Singular Value Decomposition(SVD)</b> is a factorization of <b>any</b> \(m\times n\) matrix.<br>
<h3>\[\bbox[5px,border:2px solid blue]{ \color{blue}{
    \begin{matrix}
        \\    %blank line
    \quad 
    
    % equation
    A=U\Sigma V^T
    %equation
    
    \quad\\
        \\    %blank line
    \end{matrix}
    }}
\]</h3>
Say that we have a \(m\times n\) matrix \(A\) and \(\text{Rank}(A)=r\),<br>
\[A_{m\times n}=\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\  
a_{21} & a_{22} & \cdots &  a_{2n}   \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{bmatrix}\]<br>

<blockquote style="background-color:#ECF1F6;  border-left-color:#467AAC; padding-right: 5px;">
    <u><b>IDEA:</b></u> behind SVD is to map an <b>orthonormal basis</b> in \(\mathbb{R}^n\) 
    (<a style="color: royalblue;" href="fundamental-subspaces.html#row-space">Row space</a> + <a style="color: royalblue;" href="null-space.html">Null Space</a> of matrix \(A^TA\)) to the <b>orthonormal basis</b> in \(\mathbb{R}^m\) 
    (<a style="color: royalblue;" href="column-space.html">Column space</a> + <a style="color: royalblue;" href="fundamental-subspaces.html#left-null-space">Left Null Space</a> of matrix \(AA^T\)).<br>
</blockquote><br> <!--\(\text{Rank}(A)=r\)-->

<div style="padding-bottom: 15%;">
<img style="float: right;" src="data/img/fundamental-subspaces2.png" width="50%" height="50%" alt="">
The <b>row space</b> is in \(\mathbb{R}^n\), here our row vectors are,<br>
\(\begin{bmatrix} a_{11} \\ a_{12} \\ \vdots \\ a_{1n}  \end{bmatrix}\in\mathbb{R}^n\), 
\(\begin{bmatrix} a_{21} \\ a_{22} \\ \vdots \\ a_{2n}  \end{bmatrix}\in\mathbb{R}^n\), 
\(\cdots\),
\(\begin{bmatrix} a_{m1} \\ a_{m2} \\ \vdots \\ a_{mn}  \end{bmatrix}\in\mathbb{R}^n\)<br>
The <b>column space</b> is in \(\mathbb{R}^m\), our column vectors are,<br>
\(\begin{bmatrix} a_{11} \\ a_{21} \\ \vdots \\ a_{m1}  \end{bmatrix}\in\mathbb{R}^m\), 
\(\begin{bmatrix} a_{12} \\ a_{22} \\ \vdots \\ a_{m2}  \end{bmatrix}\in\mathbb{R}^m\), 
\(\cdots\),
\(\begin{bmatrix} a_{1n} \\ a_{2n} \\ \vdots \\ a_{mn}  \end{bmatrix}\in\mathbb{R}^m\)<br>
</div>

<blockquote style="background-color:#f6f8fa; border-left-color:#fff; border-radius: 35px;">
A vector (say) \(\vec{v}\in\mathbb{R}^n\) has some portion in <b>Row Space</b> and some in <b>Left Null Space</b>, 
when we transform that vector  \(\vec{v}\in\mathbb{R}^n\) using matrix \(A\), \(A\vec{v}=\vec{u}\in\mathbb{R}^m\) now this vector
\(\vec{u}\in\mathbb{R}^m\) has some portion in <b>Column Space</b> and some in <b>Null Space</b><br>
\(\vec{u}=\vec{u}_p+\vec{u}_n\) <a style="color: royalblue;" href="complete-space.html">[Reference]</a>
</blockquote>

<li id="row-space"><b><span style="color: red;">Row space</span></b> \(\color{red}{(C(A^T))}\) (Gather Orthonormal Basis vectors for Row Space)</li>
<blockquote>
    <span style="color: red;">Row vectors</span> lives in \(n\)-dimensional vector space.<br>
    \(\text{Rank}(A)=r\) so we have \(r\) independent row vectors.<br>
    So the space spanned by row vectors is \(r\)-dimensional  vector space<b>(Row Space)</b>, so to span this whole 
    \(r\)-dimensional vector space we need \(r\) independent <b>basis vectors</b>.<br>
    Among those we choose <b>orthonormal</b> basis vectors.<br>
    
    (<i>we can get them using <a style="color: royalblue;" href="orthonormal-vectors.html#gram-schmidt">Gram Schmidt</a> method, 
    but here we will use \(A^TA\) to get them we will discuss it below)</i><br>
    (say) our <b>orthonormal</b> basis vector for <b>Row Space</b> are <span class="bb">\(\color{red}{\vec{v}_1,\vec{v}_2,\cdots,\vec{v}_r}\)</span>.<br>
</blockquote><br>
<li id="null-space"><b><span style="color: brown;">Null Space</span></b> \(\color{brown}{(N(A))}\) (Orthonormal Basis vectors of Null Space)</li>
<blockquote>
    <b><span style="color: brown;">Null Space</span></b> is perpendicular to <b>Row Space</b>(we talked about orthogonal subspaces <a style="color: royalblue;" href="orthogonal-subspaces.html">HERE</a>), 
    so every vector in Null Space is perpendicular to Row Space.<br>
    \(\text{Rank}(A)=r\) so we have \(n-r\) dependent(free) row vectors.<br>
    So the space spanned by vectors in Null Space is \((n-r)\)-dimensional  vector space<b>(Null Space)</b>, so to span this whole 
    \((n-r)\)-dimensional vector space we need \(n-r\) independent basis vectors in Null Space.<br>    
    (we don't need to find basis vectors for SVD we discuss it below in Column Space\((C(A))\) section )<br>
    (say) our <b>orthonormal</b> basis vector for <b>Null Space</b> are \(\color{brown}{\vec{v}_{r+1},\vec{v}_{r+2},\cdots,\vec{v}_n}\).<br>
</blockquote><br>

<li id="column-space"><b><span style="color: royalblue;">Column Space</span></b> \(\color{blue}{(C(A))}\) (Map the Orthonormal Basis vectors from the <b>Row Space</b> to <b>Column Space</b>)</li>
<blockquote>
    After we got our orthonormal basis vector for <b>Row Space</b> \(\color{blue}{\vec{v}_1,\vec{v}_2,\cdots,\vec{v}_n}\), 
    we map them to the <b>Column Space</b> using matrix \(A\).<br>
    \(\text{Rank}(A)=r\) so,<br>

    \[\color{}{\boxed{
        \begin{matrix}
            \\    %blank line
        \quad 
        
        % equation
        A\color{red}{\vec{v}_i}=\color{blue}{\sigma_i\vec{u}_i};\quad\forall i\in\{1,2,\cdots,r\}
        %equation
        
        \quad\\
            \\    %blank line
        \end{matrix}
        }}
    \]

    For \(i\in\{1,2,\cdots,r\}\) \(A\) maps \(\vec{v}_i\) to \(\sigma_i\vec{u}_i\) then we normalize it to get \(\vec{u}_i\)(unit vector), \(\sigma_i\) is the length of \(A\vec{v}_i\).<br>
    Now we got the <b>orthonormal basis vector</b> for our column space <span class="bb">\(\color{blue}{\vec{u}_1,\vec{u}_2,\cdots,\vec{u}_r}\)</span><br>
    <br>

    \[\color{}{\boxed{
        \begin{matrix}
            \\    %blank line
        \quad 
        
        % equation
        A\color{red}{\vec{v}_i}=\color{darkgreen}{\vec{0}};\quad\forall i\in\{r+1,r+2,\cdots,n\}
        %equation
        
        \quad\\
            \\    %blank line
        \end{matrix}
        }}
    \]

    For \(i\in\{r+1,r+2,\cdots,n\}\)  \(\vec{v}_i\) lives in the <b>Null Space</b> of \(A\)<br>
    So <span class="bb">\(\color{darkgreen}{\vec{u}_{r+1},\vec{u}_{r+2},\cdots,\vec{u}_n}\)</span> are all \(\vec{0}\)<br>
    <br>

    <b>But wait <u>WHY</u> all \(\vec{u}_i\)'s are perpendicular to each other?</b><br>
    we discussed it below in \(A^TA\) section.

</blockquote><br>

<li id="left-null-space"><b><span style="color: darkgreen;">Left Null Space</span></b> \((N(A^T))\) (Map the Orthonormal Basis vectors from the <b>Null Space</b> to <b>Left Null Space</b>)</li>
<blockquote>
    <b><span style="color: darkgreen;">Left Null Space</span></b> is perpendicular to <b>Column Space</b>(we talked about orthogonal subspaces <a style="color: royalblue;" href="orthogonal-subspaces.html">HERE</a>), 
    so every vector in Left Null Space is perpendicular to Column Space.<br>
    \(\text{Rank}(A)=r\) so we have \(m-r\) dependent(free) column vectors.<br>
    So the space spanned by vectors in Left Null Space is \((m-r)\)-dimensional  vector space<b>(Null Space)</b>, so to span this whole 
    \((m-r)\)-dimensional vector space we need \(m-r\) independent basis vectors in Left Null Space.<br>
    (say) our <b>orthonormal</b> basis vector for <b>Left Null Space</b> are \(\color{darkgreen}{\vec{u}_{r+1},\vec{u}_{r+2},\cdots,\vec{u}_n}\).<br>
</blockquote><br>
<b id="terminologies"> Terminologies </b><br>
Say <span class="bb">\(\color{red}{\vec{v}_1,\vec{v}_2,\cdots,\vec{v}_r}\)</span> are Orthonormal Basis vectors for <b><span style="color: red;">Row Space</span></b> \((C(A^T))\),<br>
and <span class="bb">\(\color{brown}{\vec{v}_{r+1},\vec{v}_{r+2},\cdots,\vec{v}_n}\)</span> are Orthonormal Basis vectors for <b><span style="color: brown;">Null Space</span></b> \((N(A^T))\)<br>
<div class="math-container">
(say) \(\mathbf{V}'=
    \begin{bmatrix}

    \color{red}{\begin{matrix}
    \vdots    & \vdots    &          & \vdots       \\  
    \vec{v}_1 & \vec{v}_2 & \cdots   &  \vec{v}_r   \\
    \vdots    & \vdots    &          & \vdots 
    \end{matrix}}
    &
    \color{brown}{\begin{matrix}
    \vdots          & \vdots        &          & \vdots \\
    \vec{v}_{r+1}   & \vec{v}_{r+2} & \cdots   & \vec{v}_n \\
    \vdots    & \vdots    &          & \vdots 
    \end{matrix}}

    \end{bmatrix}_{n\times n}
\)<br>
</div><br>
<div class="math-container">
(say) \(\mathbf{U}'=
    \begin{bmatrix}

    \color{blue}{\begin{matrix}
    \vdots    & \vdots    &          & \vdots       \\  
    \vec{u}_1 & \vec{u}_2 & \cdots   &  \vec{u}_r   \\
    \vdots    & \vdots    &          & \vdots 
    \end{matrix}}
    &
    \color{darkgreen}{\begin{matrix}
    \vdots          & \vdots        &          & \vdots \\
    \vec{u}_{r+1}   & \vec{u}_{r+2} & \cdots   & \vec{u}_n \\
    \underbrace{\vdots}_{=\vec{0}} & \underbrace{\vdots}_{=\vec{0}} & & \underbrace{\vdots}_{=\vec{0}}
    \end{matrix}}

    \end{bmatrix}_{m\times n}
\)<br>
</div><br>
<div class="math-container">
(say) \(\Sigma'=
    \begin{bmatrix}
    \begin{bmatrix}
    \sigma_1 &             &  & \huge0  \\  
             &  \sigma_2   &  &           \\
             &       & \ddots &          \\
    \huge0   &             &  &  \sigma_r \\
      
    \end  {bmatrix}_{r\times r}
    &
    \mathbf{0}_{r\times (n-r)}
    \\
    \mathbf{0}_{(n-r)\times r} & \mathbf{0}_{(n-r)\times (n-r)}
    \end{bmatrix}_{n\times n}
\)<br>
</div><br>

Mapping from \(\mathbb{R}^n\) to \(\mathbb{R}^m\) is,<br>

\[\bbox[5px,border:2px solid blue]{ \color{blue}{
    \begin{matrix}
        \\    %blank line
    \quad 
    
    % equation
    A\vec{v}_i=\sigma_i\vec{u}_i;\quad\forall i\in\{1,2,\cdots,n\}
    %equation
    
    \quad\\
        \\    %blank line
    \end{matrix}
    }}
\]

We can Write it as,
\[A\underbrace{\begin{bmatrix}
\vdots & \vdots &          & \vdots \\  
\vec{v}_1    &  \vec{v}_2   & \cdots   &  \vec{v}_n   \\
\vdots & \vdots &          & \vdots 
\end{bmatrix}}_{ V'_{_{n\times n}} }
=
\underbrace{\begin{bmatrix}
\vdots & \vdots &          & \vdots \\  
\vec{u}_1    &  \vec{u}_2   & \cdots   &  \vec{u}_n   \\
\vdots & \vdots &          & \vdots 
\end{bmatrix}}_{ U'_{m\times n} }
\underbrace{\begin{bmatrix}
\begin{bmatrix}
\sigma_1 &             &            & \huge0         \\  
         &  \sigma_2   &            &           \\
         &             & \ddots     &          \\
\huge0   &             &            &  \sigma_r \\
  
\end  {bmatrix}_{r\times r}
&
\mathbf{0}_{r\times (n-r)}
\\
\mathbf{0}_{(n-r)\times r} & \mathbf{0}_{(n-r)\times (n-r)}
\end  {bmatrix}}_{ \Sigma'_{n\times n} }\]
So,
\[
    AV'_{n\times n}=U'_{m\times n}\Sigma'_{n\times n}
\]

<li id="full-svd"><b><u>Full SVD</u></b></li>
<blockquote style="background-color:#f6f8fa; border-color: white; border-radius: 35px;">
    Say that all columns of our matrix \(A\) are independent \(\Rightarrow \text{Rank}(A)=m\), so<br>
    <li>Dimension of Row Space is: \(m\)</li>
    <li>Dimension of Null Space is: \(n-m\)</li>
    <li>Dimension of Column Space is: \(m\)</li>
    <li>Dimension of Left Null Space is: \(m-m=0\)</li>
    Our equation was,<br>
    <div class="math-container">
        \[A\underbrace{\begin{bmatrix}
        \vdots & \vdots &          & \vdots \\  
        \vec{v}_1    &  \vec{v}_2   & \cdots   &  \vec{v}_n   \\
        \vdots & \vdots &          & \vdots 
        \end{bmatrix}}_{ V'_{_{n\times n}} }
        =
        \underbrace{\begin{bmatrix}
        \vdots & \vdots &          & \vdots \\  
        \vec{u}_1    &  \vec{u}_2   & \cdots   &  \vec{u}_n   \\
        \vdots & \vdots &          & \vdots 
        \end{bmatrix}}_{ U'_{m\times n} }
        \underbrace{\begin{bmatrix}
        \begin{bmatrix}
        \sigma_1 &            &            &  \huge0         \\  
                &  \sigma_2   &             &           \\
                &             & \ddots     &          \\
        \huge0  &             &            &  \sigma_m \\
        
        \end  {bmatrix}_{m\times m}
        &
        \mathbf{0}_{m\times (n-m)}
        \\
        \mathbf{0}_{(n-m)\times m} & \mathbf{0}_{(n-m)\times (n-m)}
        \end  {bmatrix}}_{ \Sigma'_{n\times n} }\]
    </div>

    So,
        \[
            AV'_{n\times n}=U'_{m\times n}\Sigma'_{n\times n}
        \]

    And as we discussed above that \(\color{darkgreen}{\vec{u}_{r+1},\vec{u}_{r+2},\cdots,\vec{u}_n}\) are all \(\vec{0}\), 
    so we can discard those vectors from our equation so now our equation will become,<br>

    <div class="math-container">
        \[A\underbrace{\begin{bmatrix}
        \vdots & \vdots &          & \vdots \\  
        \vec{v}_1    &  \vec{v}_2   & \cdots   &  \vec{v}_n   \\
        \vdots & \vdots &          & \vdots 
        \end{bmatrix}}_{ V_{_{n\times n}} }
        =
        \underbrace{\begin{bmatrix}
        \vdots & \vdots &          & \vdots \\  
        \vec{u}_1    &  \vec{u}_2   & \cdots   &  \vec{u}_m   \\
        \vdots & \vdots &          & \vdots 
        \end{bmatrix}}_{ U_{m\times m} }
        \underbrace{\begin{bmatrix}
        \begin{bmatrix}
        \sigma_1 &             &            & \huge0          \\  
                &  \sigma_2   &             &           \\
                &             & \ddots     &          \\
        \huge0  &             &            &  \sigma_m \\
        
        \end  {bmatrix}_{m\times m}
        &
        \mathbf{0}_{m\times (n-m)}
        
        \end  {bmatrix}}_{ \Sigma_{m\times n} }\]
    </div>
    So,
    \[\bbox[5px,border:2px solid blue]{ \color{blue}{
        \begin{matrix}
            \\    %blank line
        \quad 
        
        % equation
        A_{m\times n}V_{n\times n}=U_{m\times m}\Sigma_{m\times n}
        %equation
        
        \quad\\
            \\    %blank line
        \end{matrix}
        }}
    \]

    This equation is what we call <b>Full SVD</b>
</blockquote><br><li id="reduced-svd"><b><u>Reduced SVD</u></b></li>
    <blockquote style="background-color:#f6f8fa; border-color: white; border-radius: 35px;">
    If some of the columns of our matrix \(A\) are dependent and say \(\text{Rank}(A)=r\), so<br>
    <li>Dimension of Row Space is: \(r\)</li>
    <li>Dimension of Null Space is: \(n-r\)</li>
    <li>Dimension of Column Space is: \(r\)</li>
    <li>Dimension of Left Null Space is: \(m-r\)</li>
    Our equation was,<br>
    <div class="math-container">
        \[A\underbrace{\begin{bmatrix}
        \vdots & \vdots &          & \vdots \\  
        \vec{v}_1    &  \vec{v}_2   & \cdots   &  \vec{v}_n   \\
        \vdots & \vdots &          & \vdots 
        \end{bmatrix}}_{ V'_{_{n\times n}} }
        =
        \underbrace{\begin{bmatrix}
        \vdots & \vdots &          & \vdots \\  
        \vec{u}_1    &  \vec{u}_2   & \cdots   &  \vec{u}_n   \\
        \vdots & \vdots &          & \vdots 
        \end{bmatrix}}_{ U'_{m\times n} }
        \underbrace{\begin{bmatrix}
        \begin{bmatrix}
        \sigma_1 &            &            & \huge0          \\  
                &  \sigma_2   &            &           \\
                &             & \ddots     &          \\
        \huge0  &             &            &  \sigma_m \\
        
        \end  {bmatrix}_{m\times m}
        &
        \mathbf{0}_{m\times (n-m)}
        \\
        \mathbf{0}_{(n-m)\times m} & \mathbf{0}_{(n-m)\times (n-m)}
        \end  {bmatrix}}_{ \Sigma'_{n\times n} }\]
    </div>

    So,
        \[
            AV'_{n\times n}=U'_{m\times n}\Sigma'_{n\times n}
        \]

    And as we discussed above that \(\color{darkgreen}{\vec{v}_{r+1},\vec{u}_{r+2},\cdots,\vec{u}_n}\) are all \(\vec{0}\), 
    so these vectors are redundant we can discard those vectors from our equation so now our equation will become,<br>

    <div class="math-container">
        \[A\underbrace{\begin{bmatrix}
        \vdots & \vdots &          & \vdots \\  
        \vec{v}_1    &  \vec{v}_2   & \cdots   &  \vec{v}_r   \\
        \vdots & \vdots &          & \vdots 
        \end{bmatrix}}_{ V_{_{n\times r}} }
        =
        \underbrace{\begin{bmatrix}
        \vdots & \vdots &          & \vdots \\  
        \vec{u}_1    &  \vec{u}_2   & \cdots   &  \vec{u}_r   \\
        \vdots & \vdots &          & \vdots 
        \end{bmatrix}}_{ U_{m\times r} }
        \underbrace{\begin{bmatrix}
        
        \sigma_1 &             &            & \huge0         \\  
                 &  \sigma_2   &            &           \\
                 &             & \ddots     &          \\
        \huge0   &             &            &  \sigma_r \\      
        
        \end  {bmatrix}}_{ \Sigma_{r\times r} }\]
    </div>
    So,
    \[\bbox[5px,border:2px solid blue]{ \color{blue}{
        \begin{matrix}
            \\    %blank line
        \quad 
        
        % equation
        A_{m\times n}V_{n\times r}=U_{m\times r}\Sigma_{r\times r}
        %equation
        
        \quad\\
            \\    %blank line
        \end{matrix}
        }}
    \]
    This equation is what we call <b>Reduced SVD</b>
</blockquote><br>

We have seen that we can write any \(m\times n\) matrix \(A\) as <span class="bb">\(AV=U\Sigma\)</span> where \(V\) and \(U\)
are <b>Orthonormal</b> matrix.<br>
Now the real challange is to find these \(V\) and \(U\) <b>Orthonormal</b> matrix.<br>
Once we got those \(V\) and \(U\) <b>Orthonormal</b> matrix then we can find \(A\) as,<br>
\(AV=U\Sigma\)<br>
\(A=U\Sigma V^{-1}\)<br>
And because \(V\) is an orthogonal matrix so we know that \(V^{-1}=V^T\)<br>
To achieve it we need \(V\) to be square matrix so we have to use <b>Full SVD</b> method, then we get<br>

\[\bbox[5px,border:2px solid blue]{ \color{blue}{
    \begin{matrix}
        \\    %blank line
    \quad 
    
    % equation
    A_{m\times n}=U_{m\times m}\Sigma_{m\times n}V_{n\times n}^{T}
    %equation
    
    \quad\\
        \\    %blank line
    \end{matrix}
    }}
\]<br>
<span id="find-U-and-V"></span>
Everything looks good <b>but</b> how to find these \(V\) and \(U\) <b>Orthonormal</b> matrix.<br>
<blockquote style="background-color:#ECF1F6;  border-left-color:#467AAC; padding-right: 5px;">
    <b><u>IDEA:</u></b> to find the \(V\) and \(U\) are the Matrices \(A^TA\) and \(AA^T\)
</blockquote><br><blockquote style="background-color:#ECF1F6;  border-left-color:#467AAC; padding-right: 5px;">
    <li><span style="color: red;">Row space</span> of \(A\) is same as <span style="color: red;">Row space</span> of \(A^TA\).</li>
    <i>It is easy to find the basis for <span style="color: red;">Row space</span> of \(A^TA\), so rather than finding
    basis for <span style="color: red;">Row space</span> of \(A\) we will find basis for <span style="color: red;">Row space</span> of \(A^TA\)</i>
</blockquote>

<blockquote style="background-color:#f6f8fa; border-color: white; border-radius: 35px; margin-top: 5px;">
<b>Explanation:</b><br>
\(A_{m\times n}=\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\  
a_{21} & a_{22} & \cdots &  a_{2n}   \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{bmatrix}\)<br>
Let's rewrite \(A^TA\)<br>
\(A^TA=A^T\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\  
a_{21} & a_{22} & \cdots &  a_{2n}   \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{bmatrix}\)<br>

Here we can see that we are mapping the <span style="color: red;">row vectors</span> of \(A\) onto 
the <span style="color: red;">Row Space</span> of \(A^T\).<br>

And if \(\text{Rank}(A)=r\) then \(\Rightarrow \text{Rank}(A^T)=r\) so \(A\) and \(A^T\) both are 
\(r\)-dimensions <span style="color: red;">Row Space</span>.<br>

So when we map <span style="color: red;">row vector</span> of \(A\) onto the <span style="color: red;">Row Space</span> 
of \(A^T\) then the resulting <span style="color: red;">Row Space</span> also has \(r\)-dimensions, so we can say that \(\text{Rank}(A^TA)=r\)<br>

Now we can say that <span style="color: red;">Row Space</span> of \(A\) is same as 
<span style="color: red;">Row Space</span> of \(A^TA\).<br>
<br>
\((A^TA)^T=A^TA \Rightarrow A^TA\) is symmetric matrix, so <br>
<span style="color: red;">Row Space</span> of \(A^TA=\) <span style="color: royalblue;">Column Space</span> of \(A^TA=\) <span style="color: red;">Row Space</span> of \(A\)<br>  

</blockquote><br>

<blockquote style="background-color:#ECF1F6;  border-left-color:#467AAC; padding-right: 5px;">
    <li><span style="color: royalblue;">Column Space</span> of \(A\) is same as <span style="color: royalblue;">Column Space</span> of \(AA^T\)</li>
    <i>It is easy to find the basis for <span style="color: royalblue;">Column Space</span> of \(AA^T\), so rather than finding
    basis for <span style="color: royalblue;">Column Space</span> of \(A\) we will find basis for <span style="color: royalblue;">Column Space</span> of \(AA^T\)</i>
</blockquote>

<blockquote style="background-color:#f6f8fa; border-color: white; border-radius: 35px; margin-top: 5px;">
<b>Explanation:</b><br>
\(A^T_{m\times n}=\begin{bmatrix}
a_{11} & a_{21} & \cdots & a_{m1} \\  
a_{12} & a_{22} & \cdots &  a_{m2}   \\
\vdots & \vdots & \ddots & \vdots \\
a_{1n} & a_{2n} & \cdots & a_{mn} \\
\end{bmatrix}\)<br>
Let's rewrite \(AA^T\)<br>
\(AA^T=A\begin{bmatrix}
a_{11} & a_{21} & \cdots & a_{m1} \\  
a_{12} & a_{22} & \cdots &  a_{m2}   \\
\vdots & \vdots & \ddots & \vdots \\
a_{1n} & a_{2n} & \cdots & a_{mn} \\
\end{bmatrix}\)<br>

Here we can see that we are mapping the <span style="color: royalblue;">column vectors</span> of \(A^T\) onto 
the <span style="color: royalblue;">Column Space</span> of \(A\).<br>

And if \(\text{Rank}(A)=r\) then \(\Rightarrow \text{Rank}(A^T)=r\) so \(A\) and \(A^T\) both are 
\(r\)-dimensions <span style="color: royalblue;">Column Space</span>.<br>

So when we map <span style="color: royalblue;">column vector</span> of \(A^T\) onto the <span style="color: royalblue;">Column Space</span> 
of \(A\) then the resulting <span style="color: royalblue;">Column Space</span> also has \(r\)-dimensions, so we can say that \(\text{Rank}(AA^T)=r\)<br>

Now we can say that <span style="color: royalblue;">Column Space</span> of \(A\) is same as 
<span style="color: royalblue;">Column Space</span> of \(AA^T\).<br>
<br>
\((AA^T)^T=AA^T \Rightarrow AA^T\) is symmetric matrix, so <br>
<span style="color: royalblue;">Column Space</span> of \(AA^T=\) <span style="color: red;">Row Space</span> of \(AA^T=\) <span style="color: royalblue;">Column Space</span> of \(A\)<br>
</blockquote><br>
<br><h2 id="V">Finding orthonormal Row vectors\((V)\) for matrix \(A\)</h2>

    We discussed that <span style="color: red;">Row space</span> of \(A\) is same as <span style="color: red;">Row space</span> of \(A^TA\).<br>
    So we need to find orthonormal Row vectors for matrix \(A^TA\).<br>

    \(A_{m\times n}=U_{m\times m}\Sigma_{m\times n}V_{n\times n}^{T}\)<br>
    \((\Sigma^T_{m\times n}=\Sigma_{n\times m})\)<br>
    \(\Rightarrow (A^T)_{n\times m} = V_{n\times n}\Sigma_{n\times m}U_{m\times m}^{T}\)<br>    
    \(\Rightarrow (A^T)_{n\times m}A_{m\times n} = V_{n\times n}\Sigma_{n\times m} \underbrace{U_{m\times m}^{T} U_{m\times m}}_{=\mathcal{I}_m} \Sigma_{m\times n}V_{n\times n}^{T}\)<br>

    <blockquote style="background-color:#f6f8fa; border-color: white; border-radius: 35px;">
    \(\Sigma_{n\times m}\Sigma_{m\times n}=  
    \underbrace{\begin{bmatrix}
    \begin{bmatrix}
    \sigma_1 &             &            & \huge0          \\  
            &  \sigma_2   &             &           \\
            &             & \ddots     &          \\
    \huge0  &             &            &  \sigma_m \\

    \end  {bmatrix}_{m\times m}
    \\
    \mathbf{0}_{(n-m)\times m}
    \end  {bmatrix}}_{ \Sigma_{n\times m} } 

    \underbrace{\begin{bmatrix}
    \begin{bmatrix}
    \sigma_1 &             &            & \huge0          \\  
            &  \sigma_2   &             &           \\
            &             & \ddots     &          \\
    \huge0  &             &            &  \sigma_m \\

    \end  {bmatrix}_{m\times m}
    &
    \mathbf{0}_{m\times (n-m)}
    \end  {bmatrix}}_{ \Sigma_{m\times n} } 
    \)<br>    \(\Sigma_{n\times m}\Sigma_{m\times n}=  
    \underbrace{\begin{bmatrix}
    \begin{bmatrix}
    \sigma_1^2 &             &            & \huge0          \\  
            &  \sigma_2^2   &             &           \\
            &             & \ddots     &          \\
    \huge0  &             &            &  \sigma_m^2 \\

    \end  {bmatrix}_{m\times m}
    &
    \mathbf{0}_{m\times (n-m)} \\
    \mathbf{0}_{(n-m)\times m}  & \mathbf{0}_{(n-m)\times (n-m)} \\
    \end{bmatrix}}_{ \Sigma^2_{n\times n}}
    = \Sigma^2_{n\times n}
    \)<br>
    \[\Sigma^2_{n\times n}=
    \begin{bmatrix}
    \begin{bmatrix}
    \sigma_1^2 &             &            & \huge0          \\  
            &  \sigma_2^2   &             &           \\
            &             & \ddots     &          \\
    \huge0  &             &            &  \sigma_m^2 \\

    \end  {bmatrix}_{m\times m}
    &
    \mathbf{0}_{m\times (n-m)} \\
    \mathbf{0}_{(n-m)\times m}  & \mathbf{0}_{(n-m)\times (n-m)} \\
    \end{bmatrix}_{n\times n}\]
    </blockquote><br>
    \(\Rightarrow (A^T)_{n\times m}A_{m\times n} = V_{n\times n}\Sigma^2_{n\times n}V_{n\times n}^{T}\)<br>
        Those \(3\) \(\mathbf{0}\) matrices are interesting,
        <li>\(\mathbf{0}_{m\times (n-m)}\)</li>
        <li>\(\mathbf{0}_{(n-m)\times m}\)</li>
        <li>\(\mathbf{0}_{(n-m)\times (n-m)}\)</li>
        Remember that in Full SVD we said that \(\text{Rank}(A)=m\) so there must be \(n-m\) dependent row vectors.
        and these \(\mathbf{0}\) matrices are negating those dependent vectors.<br>
        
    <br>    <blockquote>
        <a style="color: royalblue;" href="symmetric-matrices.html#p2">Remember \([A=Q\Lambda Q^T]\)</a> where \(A\) is symmetric matrix.<br>
    </blockquote>
    \(A^TA\) is a symmetric matrix, so
    <blockquote style="background-color:#ECF1F6;  border-left-color:#467AAC; padding-right: 5px;">
        Columns of \(V\) are <b>eigenvectors</b> of matrix \(A^TA\),  and \(\sigma_i^2\) are the \(i^{th}\) <b>eigenvalue</b> of matrix \(A^TA\)
    </blockquote><br>
<br><h2 id="U">Finding orthonormal Column Vectors\((U)\) for matrix \(A\)</h2>

    We discussed that <span style="color: royalblue;">Column Space</span> of \(A\) is same as <span style="color: royalblue;">Column Space</span> of \(AA^T\).<br>
    So we need to find orthonormal Column vectors for matrix \(AA^T\).<br>

    \(A_{m\times n}=U_{m\times m}\Sigma_{m\times n}V_{n\times n}^{T}\)<br>
    \((\Sigma^T_{m\times n}=\Sigma_{n\times m})\)<br>
    \(\Rightarrow (A^T)_{n\times m} = V_{n\times n}\Sigma_{n\times m}U_{m\times m}^{T}\)<br>    
    \(\Rightarrow A_{m\times n}(A^T)_{n\times m} = U_{m\times m}\Sigma_{m\times n} \underbrace{V_{n\times n}^{T} V_{n\times n}}_{=\mathcal{I}_n} \Sigma_{n\times m}U_{m\times m}^{T}\)<br>

    <blockquote style="background-color:#f6f8fa; border-color: white; border-radius: 35px;">
    \(\Sigma_{m\times n}\Sigma_{n\times m}=  
    \underbrace{\begin{bmatrix}
    \begin{bmatrix}
    \sigma_1 &             &            & \huge0          \\  
            &  \sigma_2   &             &           \\
            &             & \ddots     &          \\
    \huge0  &             &            &  \sigma_m \\

    \end  {bmatrix}_{m\times m}
    &
    \mathbf{0}_{m\times (n-m)}
    \end  {bmatrix}}_{ \Sigma_{m\times n} } 

    \underbrace{\begin{bmatrix}
    \begin{bmatrix}
    \sigma_1 &             &            & \huge0          \\  
            &  \sigma_2   &             &           \\
            &             & \ddots     &          \\
    \huge0  &             &            &  \sigma_m \\

    \end  {bmatrix}_{m\times m}
    \\
    \mathbf{0}_{(n-m)\times m}
    \end  {bmatrix}}_{ \Sigma_{n\times m} }
    \)<br>    \(\Sigma_{n\times m}\Sigma_{m\times n}=  
    \underbrace{\begin{bmatrix}
    \sigma_1^2 &             &            & \huge0          \\  
            &  \sigma_2^2   &             &           \\
            &             & \ddots     &          \\
    \huge0  &             &            &  \sigma_m^2 \\

    \end{bmatrix}}_{ \Sigma^2_{m\times m}}
    = \Sigma^2_{m\times m}
    \)<br>
    \[\Sigma^2_{m\times m}=
    \begin{bmatrix}
    \sigma_1^2 &             &            & \huge0          \\  
            &  \sigma_2^2   &             &           \\
            &             & \ddots     &          \\
    \huge0  &             &            &  \sigma_m^2 \\

    \end{bmatrix}_{m\times m}\]
    </blockquote><br>
    \(\Rightarrow A_{m\times n}(A^T)_{n\times m} = U_{m\times m}\Sigma^2_{m\times m}U_{m\times m}^{T}\)<br>
    This time there isn't any  \(\mathbf{0}\) matrix, because as we discussed,
    in Full SVD \(\text{Rank}(A)=m\) so all columns vectors and row vectors of \(U\) are independent.<br>
    <br>

    <blockquote>
        <a style="color: royalblue;" href="symmetric-matrices.html#p2">Remember \([A=Q\Lambda Q^T]\)</a> where \(A\) is symmetric matrix.<br>
    </blockquote>
    \(AA^T\) is a symmetric matrix, so
    <blockquote style="background-color:#ECF1F6;  border-left-color:#467AAC; padding-right: 5px;">
        Columns of \(U\) are <b>eigenvectors</b> of matrix \(AA^T\),  and \(\sigma_i^2\) are the \(i^{th}\) <b>eigenvalue</b> of matrix \(AA^T\)
    </blockquote>

</p>
<br>
<div onclick="location.href = 'https://join.slack.com/t/quantml-org/shared_invite/zt-hcmbg7fr-jPbVAUT_tjGPaKWU50qMYQ';" class="slack-discuss" style="color: #406b94;">Join our Slack <i class="fab fa-slack"> </i>  discussion forum</div>
<br>
<div class='buttons'>
    <button onclick="window.location.href = 'positive-definite-matrices.html';">&#x25C0;&nbsp;&nbsp;Positive Definite Matrices</button>
    <button style="float: right;" onclick="location.href = 'https://quantml.org/';"><img style="margin-top: 7px; " src="../data/icon.png" alt="logo" width="30" height="30"></button>
</div><br>

                        </section>
                    </div>
            </div>
            <!-- Highlight all -->
            <script type="text/javascript">
                SyntaxHighlighter.all()
            </script>

        <!-- Scripts -->
            <script src="../data/assets/js/jquery.min.js"></script>
            <script src="../data/assets/js/jquery.scrollex.min.js"></script>
            <script src="../data/assets/js/jquery.scrolly.min.js"></script>
            <script src="../data/assets/js/browser.min.js"></script>
            <script src="../data/assets/js/breakpoints.min.js"></script>
            <script src="../data/assets/js/util.js"></script>
            <script src="../data/assets/js/main.js"></script>

    </body>
</html>